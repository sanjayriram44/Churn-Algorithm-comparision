# ðŸ“‰ Customer Churn Prediction â€” Decision-Aware Machine Learning

## ðŸ“Œ Project Overview

This project implements an **end-to-end customer churn prediction system** using structured customer data.  
Rather than focusing only on model accuracy, the project emphasizes **decision-aware modeling**, showing how predicted probabilities are converted into actionable decisions through **threshold tuning and model comparison**.

The goal is to demonstrate how real-world machine learning systems are built, evaluated, and justified â€” not just how models are trained.

---

## ðŸŽ¯ Problem Statement

Customer churn is costly for subscription-based businesses.  
The objective of this project is to:

- Predict the **probability that a customer will churn**
- Convert predictions into **business-relevant decisions**
- Compare multiple model families fairly
- Select a final model based on evidence, not complexity

---

## ðŸ§  Key Concepts Demonstrated

- Pipeline-safe feature engineering
- Class imbalance handling
- Probability-based evaluation
- ROCâ€“AUC vs Precisionâ€“Recall tradeoffs
- Threshold tuning for decision-making
- Model comparison across different algorithms
- Interpretable conclusions

---

## ðŸ“‚ Dataset

- **Telco Customer Churn Dataset** (Kaggle)
- Contains customer demographics, subscription details, and service usage
- Binary target variable: **Churn (Yes / No)**

---

## ðŸ—ï¸ Project Structure

project/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ baseline.ipynb
â”‚ â”œâ”€â”€ 04_threshold_analysis.ipynb
â”‚ â”œâ”€â”€ 05_tree_models.ipynb
â”‚ â”œâ”€â”€ 06_boosting.ipynb
â”‚
â”œâ”€â”€ src/
â”‚ â””â”€â”€ pipelines/
â”‚ â””â”€â”€ feature_engineering.py
â”‚
â”œâ”€â”€ artifacts/
â”‚ â”œâ”€â”€ y_test.npy
â”‚ â”œâ”€â”€ y_proba.npy
â”‚ â””â”€â”€ test_indices.npy
â”‚
â””â”€â”€ README.md

---

## ðŸ”§ Feature Engineering

- Cleaned raw columns and handled missing values
- Encoded binary and categorical features
- Engineered **tenure buckets**
- Built a **custom sklearn-compatible transformer**
- Ensured no data leakage and full pipeline compatibility

All feature engineering is reusable, deterministic, and production-safe.

---

## ðŸ¤– Modeling Approach

### 1ï¸âƒ£ Baseline Model â€” Logistic Regression

- Implemented using a full sklearn pipeline
- Class imbalance handled via `class_weight="balanced"`
- Evaluation metric: **ROCâ€“AUC â‰ˆ 0.84**

This confirmed that the data contains strong predictive signal.

---

### 2ï¸âƒ£ Threshold Analysis (Decision-Making Phase)

- Used predicted probabilities instead of hard labels
- Analyzed **precisionâ€“recall tradeoffs**
- Performed threshold sweeping from 0.1 â†’ 0.9
- Selected a **high-recall operating point (~0.35)**

This step demonstrated how **one threshold choice dramatically changes outcomes**, even with the same model.

---

### 3ï¸âƒ£ Model Comparison

Models evaluated using **ROCâ€“AUC and PRâ€“AUC**:

| Model | Key Observation |
|-----|----------------|
| Logistic Regression | Strong, well-calibrated baseline |
| Decision Tree | Underperformed due to high variance |
| Random Forest | Comparable to logistic regression |
| XGBoost / LightGBM | Modest improvements (~1â€“3% PRâ€“AUC) |

**Key Insight:**  
Feature engineering and threshold tuning contributed more to performance than model complexity.

---

## ðŸ† Final Model Choice

**Logistic Regression** was selected as the final model because it offers:

- Competitive performance
- Stable and interpretable probabilities
- Easy deployment
- Clear decision control via threshold tuning

---

## ðŸ“Š Key Results

- ROCâ€“AUC â‰ˆ **0.84**
- PRâ€“AUC â‰ˆ **0.61**
- High recall achievable with controlled precision
- Clear, explainable decision tradeoffs

---

## ðŸ§© What This Project Demonstrates

This project shows the ability to:

- Build ML pipelines correctly
- Evaluate models beyond accuracy
- Make threshold-based decisions
- Compare models fairly
- Communicate tradeoffs clearly
- Think like a real ML practitioner

---

## ðŸš€ Possible Extensions

- Model explainability (coefficients / SHAP)
- Hyperparameter tuning
- Deployment as a REST API
- Cost-sensitive optimization
- Real-time inference simulation

---

## ðŸ“ Key Takeaway

> **In practical machine learning, decision-making strategy often matters more than model complexity.**

---

## ðŸ‘¤ Author

**Sanjay Sriram**  
Computer Engineering Undergraduate  
Machine Learning & AI Projects
